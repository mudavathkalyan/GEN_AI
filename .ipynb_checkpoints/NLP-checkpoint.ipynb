{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5feaf6d",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fa8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para->sen\n",
    "#para-> words(into tokens)\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "corpus=\"\"\"hello I'am kalyan,persuing B.tech,in cse.\n",
    "Iam currently learing GEN_AI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fcfecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello I'am kalyan,persuing B.tech,in cse.\n",
      "Iam currently learing GEN_AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "657da098",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)#here FULL STOP is become-> commma --->para to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4200372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hello I'am kalyan,persuing B.tech,in cse.\", 'Iam currently learing GEN_AI.']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a41017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', \"I'am\", 'kalyan', ',', 'persuing', 'B.tech', ',', 'in', 'cse', '.', 'Iam', 'currently', 'learing', 'GEN_AI', '.']\n",
      "\n",
      "Words in sentence: ['hello', \"I'am\", 'kalyan', ',', 'persuing', 'B.tech', ',', 'in', 'cse', '.']\n",
      "\n",
      "Words in sentence: ['Iam', 'currently', 'learing', 'GEN_AI', '.']\n"
     ]
    }
   ],
   "source": [
    "## para to words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words=word_tokenize(corpus)#para ->words\n",
    "\n",
    "print(words) #o/p in list\n",
    "\n",
    "\n",
    "#sentence to words\n",
    "\n",
    "# Step 2: Sentence to Words\n",
    "sentences=documents;\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(\"\\nWords in sentence:\", words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1343f2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'am',\n",
       " 'kalyan',\n",
       " ',',\n",
       " 'persuing',\n",
       " 'B',\n",
       " '.',\n",
       " 'tech',\n",
       " ',',\n",
       " 'in',\n",
       " 'cse',\n",
       " '.',\n",
       " 'Iam',\n",
       " 'currently',\n",
       " 'learing',\n",
       " 'GEN_AI',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punction also tokenizes\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e295b437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " \"I'am\",\n",
       " 'kalyan',\n",
       " ',',\n",
       " 'persuing',\n",
       " 'B.tech',\n",
       " ',',\n",
       " 'in',\n",
       " 'cse.',\n",
       " 'Iam',\n",
       " 'currently',\n",
       " 'learing',\n",
       " 'GEN_AI',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only last punctuation i.e full stop  is seperated... observe 'cse.'\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5ffca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e6bb01",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "## 1.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33785184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing-->play\n",
      "sleeping-->sleep\n",
      "eating-->eat\n",
      "dancing-->danc\n",
      "going-->go\n",
      "making-->make\n",
      "hostory-->hostori\n",
      "writes-->write\n",
      "congratulations-->congratul\n",
      "happily-->happili\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()\n",
    "words=[\"playing\",\"sleeping\",\"eating\",\"dancing\",\"going\",\"making\",\"hostory\",\"writes\",\"congratulations\",\"happily\"]\n",
    "for word in words:\n",
    "    print(word+\"-->\"+stemming.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8389e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"congratulations\")#disadvantage of stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6cd44",
   "metadata": {},
   "source": [
    "## 2.RegexStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63bfec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing--->play\n",
      "sleeping--->sleep\n",
      "eating--->eat\n",
      "dancing--->danc\n",
      "going--->go\n",
      "making--->mak\n",
      "hostory--->hostory\n",
      "writes--->write\n",
      "congratulations--->congratulation\n",
      "happily--->happily\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer=RegexpStemmer('ing|s$|ion$',min=4)\n",
    "for word in words:\n",
    "    print(word+\"--->\"+regex_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15486a",
   "metadata": {},
   "source": [
    "## 3.SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df523a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John--->john\n",
      "went--->went\n",
      "to--->to\n",
      "the--->the\n",
      "market--->market\n",
      "to--->to\n",
      "buy--->buy\n",
      "fresh--->fresh\n",
      "vegetables--->veget\n",
      "and--->and\n",
      "fruits--->fruit\n",
      "for--->for\n",
      "his--->his\n",
      "weekend--->weekend\n",
      "dinner--->dinner\n",
      "party--->parti\n",
      ".--->.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "#create snowball for english\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "for word in words:\n",
    "    print(word+\"--->\"+snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f8e43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairily--> fairli\n",
      "fairily--> fair\n"
     ]
    }
   ],
   "source": [
    "## observe diff from PoterStemmer and SnowballStemmer\n",
    "\n",
    "print(\"fairily-->\",stemming.stem('fairly'))\n",
    "\n",
    "print(\"fairily-->\",snowball.stem('fairly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c0db6",
   "metadata": {},
   "source": [
    "## 4.WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ac7b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "lemmatizer.lemmatize(\"going\",pos='v')\n",
    "\n",
    "# '\\nPOS- Noun-n\\nverb-v\\nadjective-a\\nadverb-r\\n'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8896397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing-->play\n",
      "sleeping-->sleep\n",
      "eating-->eat\n",
      "dancing-->dance\n",
      "going-->go\n",
      "making-->make\n",
      "hostory-->hostory\n",
      "writes-->write\n",
      "congratulations-->congratulations\n",
      "happily-->happily\n"
     ]
    }
   ],
   "source": [
    "#default n\n",
    "\n",
    "words=[\"playing\",\"sleeping\",\"eating\",\"dancing\",\"going\",\"making\",\"hostory\",\"writes\",\"congratulations\",\"happily\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word+\"-->\"+lemmatizer.lemmatize(word,pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ccf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c027e425",
   "metadata": {},
   "source": [
    "## #StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a00244d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"John went to the market to buy fresh vegetables and fruits for his weekend dinner party.\n",
    "He carefully selected ripe tomatoes, crisp cucumbers, and a bunch of organic carrots. The vibrant colors \n",
    "of the produce and the aromatic herbs made the market visit delightful. While picking out a few juicy apples,\n",
    "he overheard a street musician playing a soulful tune on the violin nearby.\n",
    "\n",
    "The weather was surprisingly pleasant for a late spring afternoon, with a cool breeze gently rustling the \n",
    "leaves. After finishing his shopping, John decided to take a leisurely walk through the park, where families \n",
    "were enjoying picnics and children laughed gleefully while flying colorful kites.\n",
    "\n",
    "As he approached the downtown coffee shop, he unexpectedly ran into Sarah, his childhood friend \n",
    "whom he hadn’t seen in years. They greeted each other warmly and decided to sit outside at a cozy table shaded \n",
    "by a large umbrella. The aroma of freshly brewed coffee filled the air.\n",
    "\n",
    "“It’s been forever, John!” Sarah exclaimed, smiling brightly. “I thought you’d moved out of town.”\n",
    "“I did for a while, but I’m back now,” John replied. “Life’s funny like that, isn't it?” \"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c29d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7e34900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "807d8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.words('english') # all stop words visible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80455616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b15219fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John went to the market to buy fresh vegetables and fruits for his weekend dinner party.', 'He carefully selected ripe tomatoes, crisp cucumbers, and a bunch of organic carrots.', 'The vibrant colors \\nof the produce and the aromatic herbs made the market visit delightful.', 'While picking out a few juicy apples,\\nhe overheard a street musician playing a soulful tune on the violin nearby.', 'The weather was surprisingly pleasant for a late spring afternoon, with a cool breeze gently rustling the \\nleaves.', 'After finishing his shopping, John decided to take a leisurely walk through the park, where families \\nwere enjoying picnics and children laughed gleefully while flying colorful kites.', 'As he approached the downtown coffee shop, he unexpectedly ran into Sarah, his childhood friend \\nwhom he hadn’t seen in years.', 'They greeted each other warmly and decided to sit outside at a cozy table shaded \\nby a large umbrella.', 'The aroma of freshly brewed coffee filled the air.', '“It’s been forever, John!” Sarah exclaimed, smiling brightly.', '“I thought you’d moved out of town.”\\n“I did for a while, but I’m back now,” John replied.', \"“Life’s funny like that, isn't it?”\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4e6a7e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john went market buy fresh veget fruit weekend dinner parti .', 'care select ripe tomato , crisp cucumb , bunch organ carrot .', 'vibrant color produc aromat herb made market visit delight .', 'pick juici appl , overheard street musician play soul tune violin nearbi .', 'weather surpri pleasant late spring afternoon , cool breez gentl rustl leav .', 'finish shop , john decid take leisur walk park , famili enjoy picnic children laugh gleefulli fli color kite .', 'approach downtown coff shop , unexpect ran sarah , childhood friend ’ seen year .', 'greet warm decid sit outsid cozi tabl shade larg umbrella .', 'aroma fresh brew coff fill air .', '“ ’ forev , john ! ” sarah exclaim , smile bright .', '“ thought ’ move town . ” “ , ’ back , ” john repli .', \"“ life ’ funni like , n't ? ”\"]\n"
     ]
    }
   ],
   "source": [
    "## apply stopwords and filter then apply stemming\n",
    "\n",
    "# print(sentences)\n",
    "\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])#sen->words\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) #converting list of words in to sentances back\n",
    "    \n",
    "print(sentences) # Observe the sentence  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc051878",
   "metadata": {},
   "source": [
    "## Stopword using Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8963f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john went market buy fresh veget fruit weekend dinner parti .', 'he care select ripe tomato , crisp cucumb , bunch organ carrot .', 'the vibrant color produc aromat herb made market visit delight .', 'while pick juici appl , overheard street musician play soul tune violin nearbi .', 'the weather surpris pleasant late spring afternoon , cool breez gentl rustl leav .', 'after finish shop , john decid take leisur walk park , famili enjoy picnic children laugh gleefulli fli color kite .', 'as approach downtown coffe shop , unexpect ran sarah , childhood friend ’ seen year .', 'they greet warm decid sit outsid cozi tabl shade larg umbrella .', 'the aroma fresh brew coffe fill air .', '“ it ’ forev , john ! ” sarah exclaim , smile bright .', '“ i thought ’ move town. ” “ i , i ’ back , ” john repli .', \"“ life ’ funni like , n't ? ”\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#paragraph is source of random text\n",
    "\n",
    "sentences=sent_tokenize(paragraph)#para to sentence\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])#sen->words\n",
    "    words=[snowball.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "    \n",
    "print(sentences)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51bdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03a4298",
   "metadata": {},
   "source": [
    "## Stopword using Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "67fb588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John went market buy fresh vegetable fruit weekend dinner party .', 'He carefully selected ripe tomato , crisp cucumber , bunch organic carrot .', 'The vibrant color produce aromatic herb made market visit delightful .', 'While picking juicy apple , overheard street musician playing soulful tune violin nearby .', 'The weather surprisingly pleasant late spring afternoon , cool breeze gently rustling leaf .', 'After finishing shopping , John decided take leisurely walk park , family enjoying picnic child laughed gleefully flying colorful kite .', 'As approached downtown coffee shop , unexpectedly ran Sarah , childhood friend ’ seen year .', 'They greeted warmly decided sit outside cozy table shaded large umbrella .', 'The aroma freshly brewed coffee filled air .', '“ It ’ forever , John ! ” Sarah exclaimed , smiling brightly .', '“ I thought ’ moved town. ” “ I , I ’ back , ” John replied .', \"“ Life ’ funny like , n't ? ”\"]\n"
     ]
    }
   ],
   "source": [
    "#some what accurate\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "sentences=sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word)for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "    \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4b350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248ca17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "684025a3",
   "metadata": {},
   "source": [
    "# parts of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3ee0c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"John went to the market to buy fresh vegetables and fruits for his weekend dinner party.\n",
    "He carefully selected ripe tomatoes, crisp cucumbers, and a bunch of organic carrots. The vibrant colors \n",
    "of the produce and the aromatic herbs made the market visit delightful. While picking out a few juicy apples,\n",
    "he overheard a street musician playing a soulful tune on the violin nearby.\n",
    "\n",
    "The weather was surprisingly pleasant for a late spring afternoon, with a cool breeze gently rustling the \n",
    "leaves. After finishing his shopping, John decided to take a leisurely walk through the park, where families \n",
    "were enjoying picnics and children laughed gleefully while flying colorful kites.\n",
    "\n",
    "As he approached the downtown coffee shop, he unexpectedly ran into Sarah, his childhood friend \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7a10ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/rgukt-basar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding posTags\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "885b39d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tajmahal', 'NNP'), ('is', 'VBZ'), ('very', 'RB'), ('beautiful', 'JJ'), ('one..', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sen=\"Tajmahal is very beautiful one..\"\n",
    "print(nltk.pos_tag(sen.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcea865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#postags\n",
    "\n",
    "CC\tCoordinating conjunction\tand, but, or\n",
    "CD\tCardinal number\tone, two\n",
    "DT\tDeterminer\tthe, a, an\n",
    "EX\tExistential there\tthere\n",
    "FW\tForeign word\td'accord\n",
    "IN\tPreposition/subordinating conjunction\tin, on, that\n",
    "JJ\tAdjective\tbeautiful, quick\n",
    "JJR\tAdjective, comparative\tbetter, larger\n",
    "JJS\tAdjective, superlative\tbest, largest\n",
    "LS\tList item marker\t1., A., a)\n",
    "MD\tModal auxiliary\tcan, could, may\n",
    "NN\tNoun, singular\tdog, book\n",
    "NNS\tNoun, plural\tdogs, books\n",
    "NNP\tProper noun, singular\tTajmahal, Monday\n",
    "NNPS\tProper noun, plural\tIndians, Americas\n",
    "PDT\tPredeterminer\tall, both\n",
    "POS\tPossessive ending\t's, '\n",
    "PRP\tPersonal pronoun\tI, you, he\n",
    "PRP$\tPossessive pronoun\tmy, your, his\n",
    "RB\tAdverb\tquickly, very\n",
    "RBR\tAdverb, comparative\tfaster\n",
    "RBS\tAdverb, superlative\tfastest\n",
    "RP\tParticle\tup, off\n",
    "SYM\tSymbol\t$, %, #\n",
    "TO\tto\tto\n",
    "UH\tInterjection\toh, wow\n",
    "VB\tVerb, base form\trun, jump\n",
    "VBD\tVerb, past tense\tran, jumped\n",
    "VBG\tVerb, gerund/present participle\trunning\n",
    "VBN\tVerb, past participle\teaten\n",
    "VBP\tVerb, non-3rd person singular present\trun\n",
    "VBZ\tVerb, 3rd person singular present\truns\n",
    "WDT\tWh-determiner\twhich, that\n",
    "WP\tWh-pronoun\twho, what\n",
    "WP$\tPossessive wh-pronoun\twhose\n",
    "WRB\tWh-adverb\twhere, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "816dd496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('went', 'VBD'), ('market', 'NN'), ('buy', 'VB'), ('fresh', 'JJ'), ('vegetables', 'NNS'), ('fruits', 'NNS'), ('weekend', 'NN'), ('dinner', 'NN'), ('party', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('carefully', 'RB'), ('selected', 'VBD'), ('ripe', 'NN'), ('tomatoes', 'NNS'), (',', ','), ('crisp', 'NN'), ('cucumbers', 'NNS'), (',', ','), ('bunch', 'NN'), ('organic', 'JJ'), ('carrots', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('vibrant', 'NN'), ('colors', 'NNS'), ('produce', 'VBP'), ('aromatic', 'JJ'), ('herbs', 'NNS'), ('made', 'VBN'), ('market', 'NN'), ('visit', 'NN'), ('delightful', 'NN'), ('.', '.')]\n",
      "[('While', 'IN'), ('picking', 'VBG'), ('juicy', 'NN'), ('apples', 'NNS'), (',', ','), ('overheard', 'RB'), ('street', 'NN'), ('musician', 'JJ'), ('playing', 'VBG'), ('soulful', 'JJ'), ('tune', 'NN'), ('violin', 'NN'), ('nearby', 'RB'), ('.', '.')]\n",
      "[('The', 'DT'), ('weather', 'NN'), ('surprisingly', 'RB'), ('pleasant', 'JJ'), ('late', 'JJ'), ('spring', 'NN'), ('afternoon', 'NN'), (',', ','), ('cool', 'VBP'), ('breeze', 'RB'), ('gently', 'RB'), ('rustling', 'VBG'), ('leaves', 'NNS'), ('.', '.')]\n",
      "[('After', 'IN'), ('finishing', 'VBG'), ('shopping', 'NN'), (',', ','), ('John', 'NNP'), ('decided', 'VBD'), ('take', 'VB'), ('leisurely', 'RB'), ('walk', 'NN'), ('park', 'NN'), (',', ','), ('families', 'NNS'), ('enjoying', 'VBG'), ('picnics', 'NNS'), ('children', 'NNS'), ('laughed', 'VBD'), ('gleefully', 'RB'), ('flying', 'VBG'), ('colorful', 'JJ'), ('kites', 'NNS'), ('.', '.')]\n",
      "[('As', 'IN'), ('approached', 'VBN'), ('downtown', 'IN'), ('coffee', 'NN'), ('shop', 'NN'), (',', ','), ('unexpectedly', 'RB'), ('ran', 'VBD'), ('Sarah', 'NNP'), (',', ','), ('childhood', 'NN'), ('friend', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos=nltk.pos_tag(words)\n",
    "    print(pos)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8cba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6472b341",
   "metadata": {},
   "source": [
    "# Name entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "010225b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"\"\"Elon Musk announced that Tesla will open a factory in Berlin by December 2025. \n",
    "The project is expected to create over 10,000 jobs and boost the local economy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "92a89928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc228d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence=nltk.sent_tokenize(para)\n",
    "\n",
    "words=nltk.word_tokenize(para)\n",
    "\n",
    "elements=nltk.pos_tag(words)\n",
    "\n",
    "nltk.ne_chunk(elements).draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa0487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d614390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cbf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
