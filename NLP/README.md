# NLP Learning: Text Preprocessing

## Introduction  
Before a model can understand text, it must be cleaned and structured. **Text Preprocessing** is an essential step that improves accuracy and performance.  

## Topics Covered  

### 1. Tokenization  
- Splitting text into words/sentences  
- Implemented using **NLTK** and **spaCy**  

### 2. Stopwords Removal  
- Removing common words (e.g., "is", "the", "and")  
- Helps models focus on important words  

### 3. Stemming vs. Lemmatization  
- **Stemming:** Reduces words to their root form (e.g., "running" â†’ "run")  
- **Lemmatization:** Converts words to their base form (e.g., "better" â†’ "good")  

## Why is This Important?  
- Raw text contains **noise** (extra characters, irrelevant words).  
- Preprocessing ensures **clean input** for better model accuracy.  

## Code and Implementation  
Check out my **hands-on code and examples** in this notebook:  

ðŸ”— **[GitHub Repo - NLP Notebook](https://github.com/mudavathkalyan/GEN_AI/blob/main/NLP/NLP.ipynb)**  

## How to Run  
1. Clone the repository  
   ```bash
   git clone https://github.com/mudavathkalyan/GEN_AI.git

