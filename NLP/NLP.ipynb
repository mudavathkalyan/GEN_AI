{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5feaf6d",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fa8db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "#para->sen\n",
    "#para-> words(into tokens)\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "corpus=\"\"\"hello I'am kalyan,persuing B.tech,in cse.\n",
    "Iam currently learing GEN_AI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcfecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello I'am kalyan,persuing B.tech,in cse.\n",
      "Iam currently learing GEN_AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657da098",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)#here FULL STOP is become-> commma --->para to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4200372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hello I'am kalyan,persuing B.tech,in cse.\", 'Iam currently learing GEN_AI.']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a41017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', \"I'am\", 'kalyan', ',', 'persuing', 'B.tech', ',', 'in', 'cse', '.', 'Iam', 'currently', 'learing', 'GEN_AI', '.']\n",
      "\n",
      "Words in sentence: ['hello', \"I'am\", 'kalyan', ',', 'persuing', 'B.tech', ',', 'in', 'cse', '.']\n",
      "\n",
      "Words in sentence: ['Iam', 'currently', 'learing', 'GEN_AI', '.']\n"
     ]
    }
   ],
   "source": [
    "## para to words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words=word_tokenize(corpus)#para ->words\n",
    "\n",
    "print(words) #o/p in list\n",
    "\n",
    "\n",
    "#sentence to words\n",
    "\n",
    "# Step 2: Sentence to Words\n",
    "sentences=documents;\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(\"\\nWords in sentence:\", words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1343f2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'am',\n",
       " 'kalyan',\n",
       " ',',\n",
       " 'persuing',\n",
       " 'B',\n",
       " '.',\n",
       " 'tech',\n",
       " ',',\n",
       " 'in',\n",
       " 'cse',\n",
       " '.',\n",
       " 'Iam',\n",
       " 'currently',\n",
       " 'learing',\n",
       " 'GEN_AI',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punction also tokenizes\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e295b437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " \"I'am\",\n",
       " 'kalyan',\n",
       " ',',\n",
       " 'persuing',\n",
       " 'B.tech',\n",
       " ',',\n",
       " 'in',\n",
       " 'cse.',\n",
       " 'Iam',\n",
       " 'currently',\n",
       " 'learing',\n",
       " 'GEN_AI',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only last punctuation i.e full stop  is seperated... observe 'cse.'\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5ffca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e6bb01",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "## 1.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33785184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing-->play\n",
      "sleeping-->sleep\n",
      "eating-->eat\n",
      "dancing-->danc\n",
      "going-->go\n",
      "making-->make\n",
      "hostory-->hostori\n",
      "writes-->write\n",
      "congratulations-->congratul\n",
      "happily-->happili\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()\n",
    "words=[\"playing\",\"sleeping\",\"eating\",\"dancing\",\"going\",\"making\",\"hostory\",\"writes\",\"congratulations\",\"happily\"]\n",
    "for word in words:\n",
    "    print(word+\"-->\"+stemming.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d8389e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"congratulations\")#disadvantage of stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6cd44",
   "metadata": {},
   "source": [
    "## 2.RegexStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63bfec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing--->play\n",
      "sleeping--->sleep\n",
      "eating--->eat\n",
      "dancing--->danc\n",
      "going--->go\n",
      "making--->mak\n",
      "hostory--->hostory\n",
      "writes--->write\n",
      "congratulations--->congratulation\n",
      "happily--->happily\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer=RegexpStemmer('ing|s$|ion$',min=4)\n",
    "for word in words:\n",
    "    print(word+\"--->\"+regex_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15486a",
   "metadata": {},
   "source": [
    "## 3.SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df523a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing--->play\n",
      "sleeping--->sleep\n",
      "eating--->eat\n",
      "dancing--->danc\n",
      "going--->go\n",
      "making--->make\n",
      "hostory--->hostori\n",
      "writes--->write\n",
      "congratulations--->congratul\n",
      "happily--->happili\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "#create snowball for english\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "for word in words:\n",
    "    print(word+\"--->\"+snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f8e43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairily--> fairli\n",
      "fairily--> fair\n"
     ]
    }
   ],
   "source": [
    "## observe diff from PoterStemmer and SnowballStemmer\n",
    "\n",
    "print(\"fairily-->\",stemming.stem('fairly'))\n",
    "\n",
    "print(\"fairily-->\",snowball.stem('fairly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c0db6",
   "metadata": {},
   "source": [
    "## 4.WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ac7b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "lemmatizer.lemmatize(\"going\",pos='v')\n",
    "\n",
    "# '\\nPOS- Noun-n\\nverb-v\\nadjective-a\\nadverb-r\\n'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8896397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing-->play\n",
      "sleeping-->sleep\n",
      "eating-->eat\n",
      "dancing-->dance\n",
      "going-->go\n",
      "making-->make\n",
      "hostory-->hostory\n",
      "writes-->write\n",
      "congratulations-->congratulations\n",
      "happily-->happily\n"
     ]
    }
   ],
   "source": [
    "#default n\n",
    "\n",
    "words=[\"playing\",\"sleeping\",\"eating\",\"dancing\",\"going\",\"making\",\"hostory\",\"writes\",\"congratulations\",\"happily\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word+\"-->\"+lemmatizer.lemmatize(word,pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ccf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c027e425",
   "metadata": {},
   "source": [
    "## #StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a00244d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"John went to the market to buy fresh vegetables and fruits for his weekend dinner party.\n",
    "He carefully selected ripe tomatoes, crisp cucumbers, and a bunch of organic carrots. The vibrant colors \n",
    "of the produce and the aromatic herbs made the market visit delightful. While picking out a few juicy apples,\n",
    "he overheard a street musician playing a soulful tune on the violin nearby.\n",
    "\n",
    "The weather was surprisingly pleasant for a late spring afternoon, with a cool breeze gently rustling the \n",
    "leaves. After finishing his shopping, John decided to take a leisurely walk through the park, where families \n",
    "were enjoying picnics and children laughed gleefully while flying colorful kites.\n",
    "\n",
    "As he approached the downtown coffee shop, he unexpectedly ran into Sarah, his childhood friend \n",
    "whom he hadn’t seen in years. They greeted each other warmly and decided to sit outside at a cozy table shaded \n",
    "by a large umbrella. The aroma of freshly brewed coffee filled the air.\n",
    "\n",
    "“It’s been forever, John!” Sarah exclaimed, smiling brightly. “I thought you’d moved out of town.”\n",
    "“I did for a while, but I’m back now,” John replied. “Life’s funny like that, isn't it?” \"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c29d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7e34900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "807d8627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " stopwords.words('english') # all stop words visible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80455616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b15219fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John went to the market to buy fresh vegetables and fruits for his weekend dinner party.', 'He carefully selected ripe tomatoes, crisp cucumbers, and a bunch of organic carrots.', 'The vibrant colors \\nof the produce and the aromatic herbs made the market visit delightful.', 'While picking out a few juicy apples,\\nhe overheard a street musician playing a soulful tune on the violin nearby.', 'The weather was surprisingly pleasant for a late spring afternoon, with a cool breeze gently rustling the \\nleaves.', 'After finishing his shopping, John decided to take a leisurely walk through the park, where families \\nwere enjoying picnics and children laughed gleefully while flying colorful kites.', 'As he approached the downtown coffee shop, he unexpectedly ran into Sarah, his childhood friend \\nwhom he hadn’t seen in years.', 'They greeted each other warmly and decided to sit outside at a cozy table shaded \\nby a large umbrella.', 'The aroma of freshly brewed coffee filled the air.', '“It’s been forever, John!” Sarah exclaimed, smiling brightly.', '“I thought you’d moved out of town.”\\n“I did for a while, but I’m back now,” John replied.', \"“Life’s funny like that, isn't it?”\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e6a7e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john went market buy fresh veget fruit weekend dinner parti .', 'he care select ripe tomato , crisp cucumb , bunch organ carrot .', 'the vibrant color produc aromat herb made market visit delight .', 'while pick juici appl , overheard street musician play soul tune violin nearbi .', 'the weather surprisingli pleasant late spring afternoon , cool breez gentli rustl leav .', 'after finish shop , john decid take leisur walk park , famili enjoy picnic children laugh gleefulli fli color kite .', 'as approach downtown coffe shop , unexpectedli ran sarah , childhood friend ’ seen year .', 'they greet warmli decid sit outsid cozi tabl shade larg umbrella .', 'the aroma freshli brew coffe fill air .', '“ it ’ forev , john ! ” sarah exclaim , smile brightli .', '“ i thought ’ move town. ” “ i , i ’ back , ” john repli .', \"“ life ’ funni like , n't ? ”\"]\n"
     ]
    }
   ],
   "source": [
    "## apply stopwords and filter then apply stemming\n",
    "\n",
    "# print(sentences)\n",
    "\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])#sen->words\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) #converting list of words in to sentances back\n",
    "    \n",
    "print(sentences) # Observe the sentence  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc051878",
   "metadata": {},
   "source": [
    "## Stopword using Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8963f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john went market buy fresh veget fruit weekend dinner parti .', 'he care select ripe tomato , crisp cucumb , bunch organ carrot .', 'the vibrant color produc aromat herb made market visit delight .', 'while pick juici appl , overheard street musician play soul tune violin nearbi .', 'the weather surpris pleasant late spring afternoon , cool breez gentl rustl leav .', 'after finish shop , john decid take leisur walk park , famili enjoy picnic children laugh gleefulli fli color kite .', 'as approach downtown coffe shop , unexpect ran sarah , childhood friend ’ seen year .', 'they greet warm decid sit outsid cozi tabl shade larg umbrella .', 'the aroma fresh brew coffe fill air .', '“ it ’ forev , john ! ” sarah exclaim , smile bright .', '“ i thought ’ move town. ” “ i , i ’ back , ” john repli .', \"“ life ’ funni like , n't ? ”\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#paragraph is source of random text\n",
    "\n",
    "sentences=sent_tokenize(paragraph)#para to sentence\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])#sen->words\n",
    "    words=[snowball.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "    \n",
    "print(sentences)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51bdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03a4298",
   "metadata": {},
   "source": [
    "## Stopword using Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67fb588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John went market buy fresh vegetable fruit weekend dinner party .', 'He carefully selected ripe tomato , crisp cucumber , bunch organic carrot .', 'The vibrant color produce aromatic herb made market visit delightful .', 'While picking juicy apple , overheard street musician playing soulful tune violin nearby .', 'The weather surprisingly pleasant late spring afternoon , cool breeze gently rustling leaf .', 'After finishing shopping , John decided take leisurely walk park , family enjoying picnic child laughed gleefully flying colorful kite .', 'As approached downtown coffee shop , unexpectedly ran Sarah , childhood friend ’ seen year .', 'They greeted warmly decided sit outside cozy table shaded large umbrella .', 'The aroma freshly brewed coffee filled air .', '“ It ’ forever , John ! ” Sarah exclaimed , smiling brightly .', '“ I thought ’ moved town. ” “ I , I ’ back , ” John replied .', \"“ Life ’ funny like , n't ? ”\"]\n"
     ]
    }
   ],
   "source": [
    "#some what accurate\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "sentences=sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word)for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "    \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4b350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248ca17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "684025a3",
   "metadata": {},
   "source": [
    "# parts of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ee0c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"John went to the market to buy fresh vegetables and fruits for his weekend dinner party.\n",
    "He carefully selected ripe tomatoes, crisp cucumbers, and a bunch of organic carrots. The vibrant colors \n",
    "of the produce and the aromatic herbs made the market visit delightful. While picking out a few juicy apples,\n",
    "he overheard a street musician playing a soulful tune on the violin nearby.\n",
    "\n",
    "The weather was surprisingly pleasant for a late spring afternoon, with a cool breeze gently rustling the \n",
    "leaves. After finishing his shopping, John decided to take a leisurely walk through the park, where families \n",
    "were enjoying picnics and children laughed gleefully while flying colorful kites.\n",
    "\n",
    "As he approached the downtown coffee shop, he unexpectedly ran into Sarah, his childhood friend \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "246f21b0-e332-4209-bc71-a7e278eb782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/rgukt-basar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "885b39d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/rgukt-basar/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e2b047e2-6c3f-4c84-9b78-49ff32fa2cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rgukt-basar/nltk_data', '/home/rgukt-basar/anaconda3/nltk_data', '/home/rgukt-basar/anaconda3/share/nltk_data', '/home/rgukt-basar/anaconda3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/home/rgukt-basar/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "65104d9f-2018-4225-a4b3-4d82c15be720",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/rgukt-basar/nltk_data/taggers/averaged_perceptron_tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "edef514a-9235-4312-9d1a-97fb233cdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/rgukt-basar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "51d2fd64-adca-4a9f-808e-cb1b033e6b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tajmahal', 'NNP'), ('is', 'VBZ'), ('very', 'RB'), ('beautiful', 'JJ'), ('one', 'CD'), ('..', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/rgukt-basar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('/home/rgukt-basar/nltk_data')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sen = \"Tajmahal is very beautiful one..\"\n",
    "tokens = word_tokenize(sen)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcea865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#postags\n",
    "\n",
    "CC\tCoordinating conjunction\tand, but, or\n",
    "CD\tCardinal number\tone, two\n",
    "DT\tDeterminer\tthe, a, an\n",
    "EX\tExistential there\tthere\n",
    "FW\tForeign word\td'accord\n",
    "IN\tPreposition/subordinating conjunction\tin, on, that\n",
    "JJ\tAdjective\tbeautiful, quick\n",
    "JJR\tAdjective, comparative\tbetter, larger\n",
    "JJS\tAdjective, superlative\tbest, largest\n",
    "LS\tList item marker\t1., A., a)\n",
    "MD\tModal auxiliary\tcan, could, may\n",
    "NN\tNoun, singular\tdog, book\n",
    "NNS\tNoun, plural\tdogs, books\n",
    "NNP\tProper noun, singular\tTajmahal, Monday\n",
    "NNPS\tProper noun, plural\tIndians, Americas\n",
    "PDT\tPredeterminer\tall, both\n",
    "POS\tPossessive ending\t's, '\n",
    "PRP\tPersonal pronoun\tI, you, he\n",
    "PRP$\tPossessive pronoun\tmy, your, his\n",
    "RB\tAdverb\tquickly, very\n",
    "RBR\tAdverb, comparative\tfaster\n",
    "RBS\tAdverb, superlative\tfastest\n",
    "RP\tParticle\tup, off\n",
    "SYM\tSymbol\t$, %, #\n",
    "TO\tto\tto\n",
    "UH\tInterjection\toh, wow\n",
    "VB\tVerb, base form\trun, jump\n",
    "VBD\tVerb, past tense\tran, jumped\n",
    "VBG\tVerb, gerund/present participle\trunning\n",
    "VBN\tVerb, past participle\teaten\n",
    "VBP\tVerb, non-3rd person singular present\trun\n",
    "VBZ\tVerb, 3rd person singular present\truns\n",
    "WDT\tWh-determiner\twhich, that\n",
    "WP\tWh-pronoun\twho, what\n",
    "WP$\tPossessive wh-pronoun\twhose\n",
    "WRB\tWh-adverb\twhere, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f46bf-1bd0-48e3-81c9-2fc6056fa138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "816dd496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('went', 'VBD'), ('market', 'NN'), ('buy', 'VB'), ('fresh', 'JJ'), ('vegetables', 'NNS'), ('fruits', 'NNS'), ('weekend', 'NN'), ('dinner', 'NN'), ('party', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('carefully', 'RB'), ('selected', 'VBD'), ('ripe', 'NN'), ('tomatoes', 'NNS'), (',', ','), ('crisp', 'NN'), ('cucumbers', 'NNS'), (',', ','), ('bunch', 'NN'), ('organic', 'JJ'), ('carrots', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('vibrant', 'NN'), ('colors', 'NNS'), ('produce', 'VBP'), ('aromatic', 'JJ'), ('herbs', 'NNS'), ('made', 'VBN'), ('market', 'NN'), ('visit', 'NN'), ('delightful', 'NN'), ('.', '.')]\n",
      "[('While', 'IN'), ('picking', 'VBG'), ('juicy', 'NN'), ('apples', 'NNS'), (',', ','), ('overheard', 'RB'), ('street', 'NN'), ('musician', 'JJ'), ('playing', 'VBG'), ('soulful', 'JJ'), ('tune', 'NN'), ('violin', 'NN'), ('nearby', 'RB'), ('.', '.')]\n",
      "[('The', 'DT'), ('weather', 'NN'), ('surprisingly', 'RB'), ('pleasant', 'JJ'), ('late', 'JJ'), ('spring', 'NN'), ('afternoon', 'NN'), (',', ','), ('cool', 'VBP'), ('breeze', 'RB'), ('gently', 'RB'), ('rustling', 'VBG'), ('leaves', 'NNS'), ('.', '.')]\n",
      "[('After', 'IN'), ('finishing', 'VBG'), ('shopping', 'NN'), (',', ','), ('John', 'NNP'), ('decided', 'VBD'), ('take', 'VB'), ('leisurely', 'RB'), ('walk', 'NN'), ('park', 'NN'), (',', ','), ('families', 'NNS'), ('enjoying', 'VBG'), ('picnics', 'NNS'), ('children', 'NNS'), ('laughed', 'VBD'), ('gleefully', 'RB'), ('flying', 'VBG'), ('colorful', 'JJ'), ('kites', 'NNS'), ('.', '.')]\n",
      "[('As', 'IN'), ('approached', 'VBN'), ('downtown', 'IN'), ('coffee', 'NN'), ('shop', 'NN'), (',', ','), ('unexpectedly', 'RB'), ('ran', 'VBD'), ('Sarah', 'NNP'), (',', ','), ('childhood', 'NN'), ('friend', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos=nltk.pos_tag(words)\n",
    "    print(pos)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8cba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6472b341",
   "metadata": {},
   "source": [
    "# Name entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "010225b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"\"\"Elon Musk announced that Tesla will open a factory in Berlin by December 2025. \n",
    "The project is expected to create over 10,000 jobs and boost the local economy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "92a89928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc228d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7487c14e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - '/home/rgukt-basar/nltk_data'\n    - '/home/rgukt-basar/anaconda3/nltk_data'\n    - '/home/rgukt-basar/anaconda3/share/nltk_data'\n    - '/home/rgukt-basar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/rgukt-basar/nltk_data'\n    - '/home/rgukt-basar/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m words\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mword_tokenize(para)\n\u001b[1;32m      5\u001b[0m elements\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mpos_tag(words)\n\u001b[0;32m----> 7\u001b[0m nltk\u001b[38;5;241m.\u001b[39mne_chunk(elements)\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/nltk/chunk/__init__.py:192\u001b[0m, in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    190\u001b[0m     chunker \u001b[38;5;241m=\u001b[39m ne_chunker(fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     chunker \u001b[38;5;241m=\u001b[39m ne_chunker()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/nltk/chunk/__init__.py:174\u001b[0m, in \u001b[0;36mne_chunker\u001b[0;34m(fmt)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mne_chunker\u001b[39m(fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    Load NLTK's currently recommended named entity chunker.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Maxent_NE_Chunker(fmt)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/nltk/chunk/named_entity.py:329\u001b[0m, in \u001b[0;36mMaxent_NE_Chunker.__init__\u001b[0;34m(self, fmt)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt \u001b[38;5;241m=\u001b[39m fmt\n\u001b[0;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tab_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkers/maxent_ne_chunker_tab/english_ace_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfmt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_params()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - '/home/rgukt-basar/nltk_data'\n    - '/home/rgukt-basar/anaconda3/nltk_data'\n    - '/home/rgukt-basar/anaconda3/share/nltk_data'\n    - '/home/rgukt-basar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/rgukt-basar/nltk_data'\n    - '/home/rgukt-basar/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# sentence=nltk.sent_tokenize(para)\n",
    "\n",
    "words=nltk.word_tokenize(para)\n",
    "\n",
    "elements=nltk.pos_tag(words)\n",
    "\n",
    "nltk.ne_chunk(elements).draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66282cf7",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c05a2123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e0f3ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/rgukt-\n",
      "[nltk_data]     basar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para=\"Elon Musk announced that Tesla will open a factory in Berlin by December 2025. The project is expected to create over 10,000 jobs and boost the local economy. Additionally, Tesla plans to develop new battery technologies at the Berlin facility.\"\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0cb83eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0f964bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "69516a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elon': 15,\n",
       " 'musk': 23,\n",
       " 'announced': 5,\n",
       " 'that': 31,\n",
       " 'tesla': 30,\n",
       " 'will': 34,\n",
       " 'open': 25,\n",
       " 'factory': 18,\n",
       " 'in': 19,\n",
       " 'berlin': 8,\n",
       " 'by': 10,\n",
       " 'december': 12,\n",
       " '2025': 2,\n",
       " 'the': 32,\n",
       " 'project': 28,\n",
       " 'is': 20,\n",
       " 'expected': 16,\n",
       " 'to': 33,\n",
       " 'create': 11,\n",
       " 'over': 26,\n",
       " '10': 1,\n",
       " '000': 0,\n",
       " 'jobs': 21,\n",
       " 'and': 4,\n",
       " 'boost': 9,\n",
       " 'local': 22,\n",
       " 'economy': 14,\n",
       " 'additionally': 3,\n",
       " 'plans': 27,\n",
       " 'develop': 13,\n",
       " 'new': 24,\n",
       " 'battery': 7,\n",
       " 'technologies': 29,\n",
       " 'at': 6,\n",
       " 'facility': 17}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ffc8eec9-d283-41e2-bb53-3937171102ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elon Musk announced that Tesla will open a factory in Berlin by December 2025.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c656870e-ecf0-413a-9352-4f06e05441d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac77b72-caf8-4cb2-8b51-f75edc739f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0df0d3-b688-4b3f-b6a0-9eb3597e1019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f873bab-4273-477a-a383-c1979779b35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf2756a6-9ad0-4d0c-929e-f8eb76287e31",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8b3cbf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/rgukt-basar/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b52b2e26-7f2f-42a1-b654-7b992808fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dcc8d854-e1ad-4ec4-a5cd-de6811cb007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv=api.load('word2vec-google-news-300')\n",
    "\n",
    "vec_king=wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c2dc5dde-45f8-49e9-a602-d32ff679db59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
       "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
       "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
       "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
       "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
       "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
       "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
       "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
       "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
       "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
       "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
       "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
       "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
       "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
       "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
       "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
       "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
       "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
       "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
       "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
       "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
       "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
       "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
       "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
       "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
       "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
       "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
       "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
       "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
       "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
       "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
       "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
       "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
       "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
       "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
       "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
       "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
       "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
       "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
       "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
       "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
       "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
       "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
       "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
       "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
       "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
       "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
       "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
       "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
       "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
       "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
       "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
       "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
       "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
       "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
       "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
       "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
       "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
       "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
       "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
       "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
       "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
       "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
       "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
       "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
       "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
       "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
       "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
       "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
       "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
       "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
       "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
       "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
       "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
       "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "81474c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8a1bfae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.67187500e-01, -1.21582031e-01,  2.85156250e-01,  8.15429688e-02,\n",
       "        3.19824219e-02, -3.19824219e-02,  1.34765625e-01, -2.73437500e-01,\n",
       "        9.46044922e-03, -1.07421875e-01,  2.48046875e-01, -6.05468750e-01,\n",
       "        5.02929688e-02,  2.98828125e-01,  9.57031250e-02,  1.39648438e-01,\n",
       "       -5.41992188e-02,  2.91015625e-01,  2.85156250e-01,  1.51367188e-01,\n",
       "       -2.89062500e-01, -3.46679688e-02,  1.81884766e-02, -3.92578125e-01,\n",
       "        2.46093750e-01,  2.51953125e-01, -9.86328125e-02,  3.22265625e-01,\n",
       "        4.49218750e-01, -1.36718750e-01, -2.34375000e-01,  4.12597656e-02,\n",
       "       -2.15820312e-01,  1.69921875e-01,  2.56347656e-02,  1.50146484e-02,\n",
       "       -3.75976562e-02,  6.95800781e-03,  4.00390625e-01,  2.09960938e-01,\n",
       "        1.17675781e-01, -4.19921875e-02,  2.34375000e-01,  2.03125000e-01,\n",
       "       -1.86523438e-01, -2.46093750e-01,  3.12500000e-01, -2.59765625e-01,\n",
       "       -1.06933594e-01,  1.04003906e-01, -1.79687500e-01,  5.71289062e-02,\n",
       "       -7.41577148e-03, -5.59082031e-02,  7.61718750e-02, -4.14062500e-01,\n",
       "       -3.65234375e-01, -3.35937500e-01, -1.54296875e-01, -2.39257812e-01,\n",
       "       -3.73046875e-01,  2.27355957e-03, -3.51562500e-01,  8.64257812e-02,\n",
       "        1.26953125e-01,  2.21679688e-01, -9.86328125e-02,  1.08886719e-01,\n",
       "        3.65234375e-01, -5.66406250e-02,  5.66406250e-02, -1.09375000e-01,\n",
       "       -1.66992188e-01, -4.54101562e-02, -2.00195312e-01, -1.22558594e-01,\n",
       "        1.31835938e-01, -1.31835938e-01,  1.03027344e-01, -3.41796875e-01,\n",
       "       -1.57226562e-01,  2.04101562e-01,  4.39453125e-02,  2.44140625e-01,\n",
       "       -3.19824219e-02,  3.20312500e-01, -4.41894531e-02,  1.08398438e-01,\n",
       "       -4.98046875e-02, -9.52148438e-03,  2.46093750e-01, -5.59082031e-02,\n",
       "        4.07714844e-02, -1.78222656e-02, -2.95410156e-02,  1.65039062e-01,\n",
       "        5.03906250e-01, -2.81250000e-01,  9.81445312e-02,  1.80664062e-02,\n",
       "       -1.83593750e-01,  2.53906250e-01,  2.25585938e-01,  1.63574219e-02,\n",
       "        1.81640625e-01,  1.38671875e-01,  3.33984375e-01,  1.39648438e-01,\n",
       "        1.45874023e-02, -2.89306641e-02, -8.39843750e-02,  1.50390625e-01,\n",
       "        1.67968750e-01,  2.28515625e-01,  3.59375000e-01,  1.22558594e-01,\n",
       "       -3.28125000e-01, -1.56250000e-01,  2.77343750e-01,  1.77001953e-02,\n",
       "       -1.46484375e-01, -4.51660156e-03, -4.46777344e-02,  1.75781250e-01,\n",
       "       -3.75000000e-01,  1.16699219e-01, -1.39648438e-01,  2.55859375e-01,\n",
       "       -1.96289062e-01, -2.57568359e-02, -5.41992188e-02, -2.51464844e-02,\n",
       "       -1.93359375e-01, -3.17382812e-02, -8.74023438e-02, -1.32812500e-01,\n",
       "       -2.12402344e-02,  4.33593750e-01, -5.20019531e-02,  3.46679688e-02,\n",
       "        8.00781250e-02,  3.41796875e-02,  1.99218750e-01, -2.39257812e-02,\n",
       "       -2.37304688e-01,  1.93359375e-01,  7.32421875e-02, -2.87109375e-01,\n",
       "        1.25000000e-01,  8.44726562e-02,  1.30859375e-01, -2.19726562e-01,\n",
       "       -1.61132812e-01, -2.63671875e-01, -5.46875000e-01, -2.96875000e-01,\n",
       "        3.44238281e-02, -2.87109375e-01, -1.93359375e-01, -1.61132812e-01,\n",
       "       -3.84765625e-01, -2.14843750e-01, -6.22558594e-03, -1.27929688e-01,\n",
       "       -1.00097656e-01, -6.21093750e-01,  3.78906250e-01, -4.58984375e-01,\n",
       "        1.44531250e-01, -9.13085938e-02, -3.08593750e-01,  2.23632812e-01,\n",
       "        7.86132812e-02, -2.16796875e-01,  8.78906250e-02, -1.66992188e-01,\n",
       "        1.14746094e-02, -2.53906250e-01, -6.25000000e-02,  6.04248047e-03,\n",
       "        1.56250000e-01,  4.37500000e-01, -2.23632812e-01, -2.32421875e-01,\n",
       "        2.75390625e-01,  2.39257812e-01,  4.49218750e-02, -7.51953125e-02,\n",
       "        5.74218750e-01, -2.61230469e-02, -1.21582031e-01,  2.44140625e-01,\n",
       "       -3.37890625e-01,  8.59375000e-02, -7.71484375e-02,  4.85839844e-02,\n",
       "        1.43554688e-01,  4.25781250e-01, -4.29687500e-02, -1.08398438e-01,\n",
       "        1.19628906e-01, -1.91406250e-01, -2.12890625e-01, -2.87109375e-01,\n",
       "       -1.14746094e-01, -2.04101562e-01, -2.06298828e-02, -2.53906250e-01,\n",
       "        8.25195312e-02, -3.97949219e-02, -1.57226562e-01,  1.34765625e-01,\n",
       "        2.08007812e-01, -1.78710938e-01, -2.00195312e-02, -8.34960938e-02,\n",
       "       -1.20605469e-01,  4.29687500e-02, -1.94335938e-01, -1.32812500e-01,\n",
       "       -2.17285156e-02, -2.35351562e-01, -3.63281250e-01,  1.51367188e-01,\n",
       "        9.32617188e-02,  1.63085938e-01,  1.02050781e-01, -4.27734375e-01,\n",
       "        2.83203125e-01,  2.74658203e-04, -3.20312500e-01,  1.68457031e-02,\n",
       "        4.06250000e-01, -5.24902344e-02,  7.91015625e-02, -1.41601562e-01,\n",
       "        5.27343750e-01, -1.26953125e-01,  4.74609375e-01, -6.64062500e-02,\n",
       "        3.41796875e-01, -1.78710938e-01,  3.69140625e-01, -2.05078125e-01,\n",
       "        5.82885742e-03, -1.84570312e-01, -8.88671875e-02, -1.81640625e-01,\n",
       "       -4.80957031e-02,  4.39453125e-01,  2.12890625e-01, -3.07617188e-02,\n",
       "        9.32617188e-02,  2.40234375e-01,  2.39257812e-01,  2.51953125e-01,\n",
       "       -1.98974609e-02,  1.24511719e-01, -4.73632812e-02, -2.13623047e-02,\n",
       "        3.12500000e-02,  3.05175781e-02,  2.79296875e-01,  9.08203125e-02,\n",
       "       -2.02148438e-01, -2.19726562e-02, -2.63671875e-01,  8.78906250e-02,\n",
       "       -1.07421875e-01, -2.49023438e-01, -1.22070312e-02,  1.73828125e-01,\n",
       "       -9.91210938e-02,  7.27539062e-02,  2.59765625e-01, -4.60937500e-01,\n",
       "        3.59375000e-01, -2.25585938e-01,  1.87988281e-02, -2.19726562e-01,\n",
       "       -2.08984375e-01, -1.51367188e-01,  8.64257812e-02,  1.11694336e-02,\n",
       "        6.93359375e-02, -2.99072266e-02,  1.43554688e-01,  1.89453125e-01,\n",
       "       -1.32812500e-01,  4.72656250e-01, -1.40625000e-01, -2.52685547e-02,\n",
       "        1.91406250e-01, -2.63671875e-01, -1.39648438e-01,  1.09375000e-01,\n",
       "        1.97753906e-02,  2.49023438e-01, -1.42578125e-01,  4.15039062e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['cricket']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed46d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similarity('cricket','sport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=wv['king']-wv['man']+wv['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61799821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c397a4-57b2-4bed-9ef3-d4c04bf0c9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
